{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sjagkoo7/kaggle-learning-diary?scriptVersionId=139062068\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"a0f67397","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-06T08:58:13.967452Z","iopub.status.busy":"2023-08-06T08:58:13.966976Z","iopub.status.idle":"2023-08-06T08:58:13.980262Z","shell.execute_reply":"2023-08-06T08:58:13.978834Z"},"papermill":{"duration":0.025414,"end_time":"2023-08-06T08:58:13.983274","exception":false,"start_time":"2023-08-06T08:58:13.95786","status":"completed"},"tags":[]},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"faa8d425","metadata":{"papermill":{"duration":0.004399,"end_time":"2023-08-06T08:58:13.992886","exception":false,"start_time":"2023-08-06T08:58:13.988487","status":"completed"},"tags":[]},"source":[">  # **Learning from**\n","# 1.**EDA of IPL Matches_Am** notebook\n","# 2.**Feature-Engineering IPL 2008 to 2020 _Am** notebook\n","# 3. **Visualization IPL 2008 to 2020_Am** notebook\n","* matches['city'].replace(to_replace='Bengaluru',value='Bangalore',inplace=True) **replace Bengaluru with Banglore**\n","* win_per_season = matches.groupby('season').winner.value_counts() **grouby**\n","* win_per_season_df=pd.DataFrame(columns=['season','team','no_of_time_won'])\n","\n","     year=2008\n","     for items in win_per_season.items():\n","        if(items[0][0]==year):\n","            print(items)\n","            temp=pd.DataFrame({'season':[items[0][0]],'team':[items[0][1]],'no_of_time_won':[items[1]]})\n","            win_per_season_df=win_per_season_df.append(temp,ignore_index=True)\n","            year=year+1\n","> **loc vs iloc**\n","    * loc is label-based, which means that you have to specify rows and columns based on their row and column labels.\n","    * iloc is integer position-based, so you have to specify rows and columns by their integer position values (0-based integer position). \n","    * loc[row_label, column_label]\n","    * iloc[row_position, column_position]\n","    * 1.Selecting via a single value 2.Selecting via a list of values 3.Selecting a range of data via slice 4.Selecting via conditions and callable\n","    * https://towardsdatascience.com/how-to-use-loc-and-iloc-for-selecting-data-in-pandas-bd09cb4c3d79\n","* idxmax() , idxmin() , ge() --> greater than equal to"]},{"cell_type":"markdown","id":"25e81b66","metadata":{"papermill":{"duration":0.004303,"end_time":"2023-08-06T08:58:14.001913","exception":false,"start_time":"2023-08-06T08:58:13.99761","status":"completed"},"tags":[]},"source":["> # 4.  Learning from **Estes Park Weather EDA_Am** notebook\n","* avg_50_75_diff =climate['Average temperature (°F)'].quantile(q=0.75)-climate['Average temperature (°F)'].quantile(q=0.5)\n","* print('avg_50_75_diff :', round(avg_50_75_diff,2)) -- count calculate quantile"]},{"cell_type":"markdown","id":"3254135d","metadata":{"papermill":{"duration":0.005556,"end_time":"2023-08-06T08:58:14.012337","exception":false,"start_time":"2023-08-06T08:58:14.006781","status":"completed"},"tags":[]},"source":["> # 5. Learning from **Cryptocurrencies_Am** notebook\n","* A1=df.head(3).to_dict('list')\n","* list_1=list_1['Currency_Name'].to_list()\n","* set_xlabel,set_ylabel,set_title,legend,set_xticks,set_xticklabels,set_yticks,set_yticklabels"]},{"cell_type":"markdown","id":"4a922989","metadata":{"papermill":{"duration":0.004805,"end_time":"2023-08-06T08:58:14.02231","exception":false,"start_time":"2023-08-06T08:58:14.017505","status":"completed"},"tags":[]},"source":["> # 6. Learning from **Boston House Prices_Am** notebook\n","* from sklearn import metrics\n","* from sklearn.model_selection import train_test_split\n","* X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=4)\n","> **Linear Regression**\n","* from sklearn.linear_model import LinearRegression -- Import library for Linear Regression\n","* lr=LinearRegression() -- Create a Linear regressor\n","* lr.fit(X_train,y_train) -- Train the model using the training sets\n","* y_pred=lr.predict(X_test) -- Model prediction on test data\n","> **Linear Regression model Evaluation**\n","* print('R^2:',metrics.r2_score(y_test,y_pred_test))\n","* print('Adjusted R^2 :',1-((1-metrics.r2_score(y_test,y_pred_test))*(len(y_test)-1))/(len(y_test)-len(house.columns)-1))\n","* print('MAE:',metrics.mean_absolute_error(y_test,y_pred_test))\n","* print('MSE:',metrics.mean_squared_error(y_test,y_pred_test))\n","* print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred_test)))"]},{"cell_type":"markdown","id":"f4187e94","metadata":{"papermill":{"duration":0.004451,"end_time":"2023-08-06T08:58:14.031506","exception":false,"start_time":"2023-08-06T08:58:14.027055","status":"completed"},"tags":[]},"source":["> # 7. Learning from **EDA of Top 500 Indian Cities Population** notebook"]},{"cell_type":"markdown","id":"14cb5b33","metadata":{"papermill":{"duration":0.004433,"end_time":"2023-08-06T08:58:14.040722","exception":false,"start_time":"2023-08-06T08:58:14.036289","status":"completed"},"tags":[]},"source":["> # 8. Learning from **Data Analysis+Regression+Classification** notebook\n","* **Data Wrangling**    \n","* def transform_categorical_column(data,column_name):\n","    categories=data[column_name].value_counts().index.to_list()\n","    map_cat={k:v  for v,k in enumerate(categories)}\n","    rev_map_cat={v:k for k,v in map_cat.items()}\n","    data[column_name]=data[column_name].map(map_cat)\n","    return data,map_cat,rev_map_cat\n","*  data, map_gender, reverse_map_gender=transform_categorical_column(data, 'gender')\n","> **Classification- Decision Tree Classification**\n","* from sklearn.tree import DecisionTreeClassifier\n","* from sklearn.metrics import accuracy_score,classification_report\n","* dtree=DecisionTreeClassifier(criterion=\"entropy\")\n","* dtree.fit(X_train,y_train)\n","* y_pred=dtree.predict(X_test)\n","> **Classification model Evaluation**\n","* accuracy_score(y_test,y_pred)\n","* print(classification_report(y_test,y_pred))"]},{"cell_type":"markdown","id":"075dc80f","metadata":{"papermill":{"duration":0.004475,"end_time":"2023-08-06T08:58:14.049842","exception":false,"start_time":"2023-08-06T08:58:14.045367","status":"completed"},"tags":[]},"source":["> # 9. Learning from **Supermarket Sales EDA** notebook\n","* **Pie chart using Matplotlib**\n","* fig=plt.figure(figsize=(10,5))\n","* ax=fig.add_subplot(111)\n","* ax.set(title='Product line')\n","* plt.pie(x=sales['Product line'].value_counts(),autopct='%.1f%%',labels=sales['Product line'].value_counts().index)\n","* plt.show()\n","> **Datetime**\n","* sales['Date2']=pd.to_datetime(sales['Date1'])\n","* sales['Day']=sales.Date2.dt.day\n","* sales['Month']=sales.Date2.dt.month\n","* sales['Year']=sales.Date2.dt.year\n","* sales['Weekday']=sales.Date2.dt.dayofweek\n","* sales['Hour']=sales.Time1.dt.hour"]},{"cell_type":"markdown","id":"63c40330","metadata":{"papermill":{"duration":0.004324,"end_time":"2023-08-06T08:58:14.058928","exception":false,"start_time":"2023-08-06T08:58:14.054604","status":"completed"},"tags":[]},"source":["> # 10. Learning from **heart-failure-prediction_Am** notebok\n","* plt.subplot(x,y,z)\n","        x - row\n","        y - column\n","        z - plot number\n","        total graph will be = x * y and z will be squence number of graph\n","* plt.tight_layout()\n","* For not Tree based Machine Learning Algorithms the best way to go will be to use One-Hot Encoding \n","* For Tree based Machine Learning Algorithms the best way to go is with Label Encoding \n","* Using Decision Tree based Algorithm does not require feature scaling, and works great also in presence of categorical columns without ONE_HOT Encoding"]},{"cell_type":"markdown","id":"d77cb379","metadata":{"papermill":{"duration":0.004324,"end_time":"2023-08-06T08:58:14.067909","exception":false,"start_time":"2023-08-06T08:58:14.063585","status":"completed"},"tags":[]},"source":["> # 11. Learning from **Apps Installs EDA** notebook\n","* https://www.programiz.com/python-programming/datetime/strptime\n","* https://www.programiz.com/python-programming/datetime/strftime\n","* https://jovian.com/sanketchavan5595/exploratory-data-analysis-on-google-playstore-apps\n","> **Convert 10,00000+ object to 1000000 number**\n","* data['Installs']=data['Installs'].apply(lambda x:x.replace(',',''))\n","* data['Installs']=data['Installs'].apply(lambda x:x.replace('+',''))\n","* data['Installs']=pd.to_numeric(data['Installs'])"]},{"cell_type":"markdown","id":"ed93a554","metadata":{"papermill":{"duration":0.004303,"end_time":"2023-08-06T08:58:14.07695","exception":false,"start_time":"2023-08-06T08:58:14.072647","status":"completed"},"tags":[]},"source":["> # 12. Learning from **Policy Prediction Am** notebook\n","* import ydata_profiling\n","* tr_data.profile_report(title='Customer Report',progress_bar=False)"]},{"cell_type":"markdown","id":"5efb35c4","metadata":{"papermill":{"duration":0.004316,"end_time":"2023-08-06T08:58:14.085938","exception":false,"start_time":"2023-08-06T08:58:14.081622","status":"completed"},"tags":[]},"source":["> # 13. Learning from **Car Evaluation|Decision Tree,Random Forest,SVM** notebook\n","* **Ordinal Encoding**\n","* Ordinal encoding is used when the categorical data has a natural order or ranking\n","* For example, if the categories are \"Low,\" \"Medium,\" and \"High,\" ordinal encoding may assign them integer values like 1, 2, and 3, respectively.\n","* import category_encoders as ce \n","* encoder=ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']) -- cols -- is pre-define variable \n","* X=encoder.fit_transform(X)\n","> **Confusion Matrix**\n","* from sklearn.metrics improt confusion_matrix\n","* cm=confusion_matrix(y_test,y_pred_test)\n","* print(cm)\n","> **Feature Importance - Using Random Forest Classifier**\n","* feature_importance=pd.Series(randomclassifier.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n","* feature_importance\n","> **SVM - Support Vector Machine**\n","* Applicable for both Regression & Classification\n","* Kernel types - Linear - linear , Polynomial - , RBF - Radial Basis Function (RBF) or Gaussian Kernel - poly and Sigmoid - sigmoid\n","* from sklearn.svm import SVC\n","* svm_linear= SVC(kernel='linear') -- linear kernel\n","* svm_linear.fit(X_train,y_train)\n","* y_pred_test_svm=svm_linear.predict(X_test)"]},{"cell_type":"markdown","id":"77059400","metadata":{"papermill":{"duration":0.004334,"end_time":"2023-08-06T08:58:14.094964","exception":false,"start_time":"2023-08-06T08:58:14.09063","status":"completed"},"tags":[]},"source":["> # 14. Learning from **Binary Classification of Machine Failures S3E17** notebook\n","* **Ensemble - Randomized Trees - Random Forest Classifier**\n","* from sklearn.ensemble import RandomForestClassifier\n","* dtree=RandomForestClassifier(n_estimators=100,criterion='entropy')\n","* dtree.fit(X_train,y_train)\n","* y_pred=dtree.predict(X_test)\n","> **Label Encoding - Used with Tree Based Algorthms**\n","* Label encoding is a simpler form of encoding where each unique category is assigned a unique integer value.\n","* from sklearn.preprocessing import LabelEncoder\n","* le = LabelEncoder()\n","* df['Type'] = le.fit_transform(df['Type'])\n","* df.head()\n","> **One Hot Encoding - Used with not Tree based  Algorithms**\n","* One-hot encoding creates binary columns for each category in the original data and represents the presence of a category with a 1 and the absence with a 0.\n","* dummy_variables=pd.get_dummies(data, columns=categorical_columns, drop_first=False) -- categorical_columns is all categorical column name of dataset\n","* dummy_variables.head()\n","> **Feature Scaling**\n","* https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n","* from sklearn.preprocessing import RobustScaler , StandardScaler , MinMaxScaler\n","* **RobustScaler**\n","* scaler = RobustScaler()\n","* df[numerical_features] = scaler.fit_transform(df[numerical_features])  -- numerical_features : numerical columns\n","* Use RobustScaler if you want to reduce the effects of outliers\n","* RobustScaler does not scale the data into a predetermined interval like MinMaxScaler\n","* RobustScaler transforms the feature vector by subtracting the median and then dividing by the interquartile range (75% value — 25% value)\n","* **StandardScaler**\n","* scaler = StandardScaler()\n","* df[numerical_features] = scaler.fit_transform(df[numerical_features]) -- numerical_features : numerical columns\n","* StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance.\n","* **MinMaxScaler**\n","* scaler = MinMaxScaler()\n","* df[numerical_features] = scaler.fit_transform(df[numerical_features]) -- numerical_features : numerical columns\n","* or each value in a feature, MinMaxScaler subtracts the minimum value in the feature and then divides by the range. The range is the difference between the original maximum and original minimum.\n","* MinMaxScaler doesn’t reduce the importance of outliers.\n","* The default range for the feature returned by MinMaxScaler is 0 to 1.\n","> **Handle duplicates**\n","* duplicate_rows_data = data[data.duplicated()]\n","* data= data.drop_duplicates()\n"]},{"cell_type":"markdown","id":"7a0bc44d","metadata":{"papermill":{"duration":0.004262,"end_time":"2023-08-06T08:58:14.103839","exception":false,"start_time":"2023-08-06T08:58:14.099577","status":"completed"},"tags":[]},"source":["> # 15. Learning from **Multi-Label Classification Enzyme Dataset** notebook\n","* Multi-CLassification  - predict output for each target seperatley by applying suitable  binary classification aglo  then merge the output in two differnt column as like target in training dataset. \n","* **Ensemble - Boosting Methods - Gradient Boosting Classifier, AdaBoost Classifier,CatBoost Classifier**\n","* Gradient Boosting Classifier - is an ensemble machine learning algorithm that creates a strong predictive model by combining the predictions of multiple weak models, typically decision trees.\n","* CatBoost Classifier - is a machine learning algorithm that uses gradient boosting on decision trees with a special emphasis on handling categorical variables. It stands for \"Category\" and \"Boosting\".\n","* CatBoost can automatically handle categorical variables and does not require extensive data preprocessing like other machine learning algorithms.\n","* AdaBoost Classifier or Adaptive Boosting - is one of the simplest boosting algorithms.AdaBoost is best used to boost the performance of decision trees on binary classification problems. \n","> **AdaBoost Classifier Algo**\n","* from sklearn.ensemble import AdaBoostClassifier\n","* model_EC1=AdaBoostClassifier(n_estimators=200, learning_rate=0.1)\n","* model_EC1.fit(train_df_EC1_EC2,y_train_ec1)\n","* y_pred_ec1=model_EC1.predict(test_df_EC1_EC2)\n"]},{"cell_type":"markdown","id":"a7e911fd","metadata":{"papermill":{"duration":0.004354,"end_time":"2023-08-06T08:58:14.112806","exception":false,"start_time":"2023-08-06T08:58:14.108452","status":"completed"},"tags":[]},"source":["># 16.Learning from **Forecasting Mini-Course Sales S3E19** notebook\n","* **time series pandas method**\n","* resample - Convenience method for frequency conversion and resampling of time series\n","* to_period - Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency \n","* to_timestamp - Cast to DatetimeIndex of timestamps, at beginning of period.\n","> **Create a FacetGrid to make a separate line plot for each product**\n","* col_wrap -- means number of columns in grid\n","* aspect -- it will seperate the graphs from overlapping\n","* g=sns.FacetGrid(monthly_sales_country_product,col='product',col_wrap=3,height=4,aspect=1.5)\n","* g.map_dataframe(sns.lineplot,x='date',y='num_sold',hue='country')\n","* g.set_axis_labels('Date', 'Number of Products Sold')\n","* g.add_legend()\n","* g.set_axis_labels('Date', 'Number of Products Sold')\n","* g.fig.suptitle('Trend of Sales Over Time for Each Country and Product')\n","* plt.subplots_adjust(top=0.9) -- it will adjust the graph\n","> **Ensemble - LGBMRegressor - Light gradient boosting model**\n","* one of the best model to work with time series test data\n","* from lightgbm import LGBMRegressor\n","* lgbm=LGBMRegressor(n_estimators=2000,learning_rate=0.01,random_state=42)\n","* lgbm.fit(X_train_scaled,y_train)\n","* y_pred=lgbm.predict(X_test_scaled)"]},{"cell_type":"markdown","id":"d26015ce","metadata":{"papermill":{"duration":0.004252,"end_time":"2023-08-06T08:58:14.121609","exception":false,"start_time":"2023-08-06T08:58:14.117357","status":"completed"},"tags":[]},"source":["> # 17. Learning from **Time Series Crypto Currencies** notebook \n","* Stationary-  https://www.analyticsvidhya.com/blog/2021/06/statistical-tests-to-check-stationarity-in-time-series-part-1/\n","* Trend-Exponential Weighted Moving Average (EWMA)\n","    * https://towardsdatascience.com/achieving-stationarity-with-time-series-data-abd59fd8d5a0\n","    * https://medium.com/codex/simple-moving-average-and-exponentially-weighted-moving-average-with-pandas-57d4a457d363\n","> **Stationarity** -\n","* A Stationary series is one whose statistical properties such as mean, variance, covariance, and standard deviation do not vary with time, or these stats properties are not a function of time.\n","* **Strict Stationarity** - Satisfies the mathematical definition of a stationary process. Mean, variance & covariance are not a function of time.\n","* **Seasional Stationarity** -Series exhibiting seasonality.\n","* **Trend Stationarity** - Series exhibiting trend.\n","> **Trend Removal**\n","* *Differencing*\n","* *Moving Average Smoothing - Simple Moving Average (SMA) or Weighted Moving Average (WMA)*\n","* *Exponential Weighted Moving Average (EWMA)*\n","    * ewma = ts_log.ewm(halflife=halflife, ignore_na=False, min_periods=0, adjust=True) -- ts_log is timeseries column i.e. like price , age , e.t.c. Keep in mind timestamp is not the timeseries data\n","* *Seasonal Decomposition* - For both trend and Seasional removal. Techniques - Seasonal Decomposition of Time Series (STL) or Seasonal and Trend decomposition using LOESS (STL)\n","    * from  statsmodels.tsa.seasonal import seasonal_decompose\n","    * decomposition = seasonal_decompose(ts_log)\n","    * trend = decomposition.trend\n","    * seasonal = decomposition.seasonal\n","    * residual = decomposition.resid\n","> **Stationarity - Test Algorithm**\n","* **ADF - Agumented Dicky - Fuller tests** \n"," * from statsmodels.tsa.stattools import adfuller\n"," * dftest=adfuller(timeseries , autolag='AIC')\n","* **Kwiatkowski–Phillips–Schmidt–Shin (KPSS) tests**\n"," * from statsmodels.tsa.stattools import kpss\n"," * dftest=kpss(timeseries , regression='c',nlags='auto')\n","> **Stationarity or Non - stationarity - P Value**\n","* Null Hypothesis(HO) - Time Series is Non-Stationarity - Series has unit root\n","* Alternate Hypothesis(HA) -  Time Series is Stationarity - Series has not unit root\n","> **Conditions to Reject Null Hypothesis(HO)**\n","- If **Test statistic < Critical Value and p-value < 0.05** – Reject Null Hypothesis(HO), i.e., time series does not have a unit root, meaning it is *stationary*. It does not have a time-dependent structure."]},{"cell_type":"markdown","id":"a4c7b144","metadata":{"papermill":{"duration":0.004309,"end_time":"2023-08-06T08:58:14.130443","exception":false,"start_time":"2023-08-06T08:58:14.126134","status":"completed"},"tags":[]},"source":["> # **General**\n","* 40 of the Best Beginner-Friendly Kaggle Notebooks to Learn Exploratory Data Analysis (EDA) -https://medium.com/@ebrahimhaqbhatti516/40-of-the-best-beginner-friendly-kaggle-notebooks-to-learn-exploratory-data-analysis-eda-6e45760646aa"]},{"cell_type":"markdown","id":"f2ac124e","metadata":{"papermill":{"duration":0.004298,"end_time":"2023-08-06T08:58:14.139396","exception":false,"start_time":"2023-08-06T08:58:14.135098","status":"completed"},"tags":[]},"source":["> # **Visualization Using Seabron and Matplotlib**\n","* notebook - 1,2,3,4,13,14,15,16,17 done\n","* sns.lineplot()\n","* sns.countplot()\n","* sns.barplot()\n","* sns.displot()\n","* sns.histplot()\n","* sns.catplot()\n","* plt.pie()\n","* sns.heatmap()\n","* sns.boxplot()\n","* sns.displot()\n","* sns.histplot()\n","* sns.violinplot()\n"]},{"cell_type":"markdown","id":"b19cfde1","metadata":{"papermill":{"duration":0.004384,"end_time":"2023-08-06T08:58:14.148315","exception":false,"start_time":"2023-08-06T08:58:14.143931","status":"completed"},"tags":[]},"source":["> # **ML Algorithm**\n","* **Regression**\n","    * Linear Regression\n","    * Ensemble - LGBMRegressor - Light gradient boosting model \n","    * SVM - Support Vactor Machine - SVR , Nu-SVR , LinearSVR - R - means Regression\n","* **Classification**\n","    * Tree - Decision Tree\n","    * Ensemble - Randomized Trees - Random Forest Classifier\n","    * Ensemble - Boosting Methods - AdaBoost Classifier\n","    * SVM - Support Vector Machine - Linear,Polynomial , RBF and Sigmoid - Kernel\n","    "]},{"cell_type":"markdown","id":"15e366aa","metadata":{"papermill":{"duration":0.004333,"end_time":"2023-08-06T08:58:14.157247","exception":false,"start_time":"2023-08-06T08:58:14.152914","status":"completed"},"tags":[]},"source":["> # **Steps in a Machine Learning Project**\n","* Defining the Problem\n","* Obtaining the Source Data\n","* Preparing Data for Machine Learning Algorithms\n","    > **Data Preprocessing**\n","    * Handling Null Values || Data Cleaning\n","    * Handling Categorical Variables || One Hot Encoding | Label Encoding | Ordinal Encoding | Dummy Encoding\n","    * Feature Scaling || Feature Engineering || Feature Selection\n","* Understanding Data Through Visualization\n","* Chossing an algorithm\n","* Builiding the Model\n","* Fine-tuning the Model\n","     > **Model Evaluation**\n","* Use the best model"]},{"cell_type":"markdown","id":"ec331cee","metadata":{"papermill":{"duration":0.004293,"end_time":"2023-08-06T08:58:14.166731","exception":false,"start_time":"2023-08-06T08:58:14.162438","status":"completed"},"tags":[]},"source":["#### \n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":12.571295,"end_time":"2023-08-06T08:58:14.994503","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-08-06T08:58:02.423208","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}