{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sjagkoo7/kaggle-learning-diary-machine-learning?scriptVersionId=198863736\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"3f7a5792","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-30T05:34:57.523252Z","iopub.status.busy":"2024-09-30T05:34:57.522396Z","iopub.status.idle":"2024-09-30T05:34:57.610094Z","shell.execute_reply":"2024-09-30T05:34:57.608751Z"},"papermill":{"duration":0.104299,"end_time":"2024-09-30T05:34:57.614044","exception":false,"start_time":"2024-09-30T05:34:57.509745","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/ipl-complete-dataset-20082020/matches.csv\n","/kaggle/input/ipl-complete-dataset-20082020/deliveries.csv\n","/kaggle/input/policy/train.csv\n","/kaggle/input/policy/test.csv\n","/kaggle/input/play-store-apps/googleplaystore.csv\n","/kaggle/input/play-store-apps/googleplaystore_user_reviews.csv\n","/kaggle/input/heart-failure-prediction/heart.csv\n","/kaggle/input/fuel-gas-emission/train.csv\n","/kaggle/input/fuel-gas-emission/test.csv\n","/kaggle/input/bmi-body-mass-index/bmi_train.csv\n","/kaggle/input/bmi-body-mass-index/bmi_validation.csv\n","/kaggle/input/boston-house-prices/housing.csv\n","/kaggle/input/climate-dataset/data.csv\n","/kaggle/input/car-evaluation-data-set/car_evaluation.csv\n","/kaggle/input/crypto-currencies-data-set/data1.csv\n","/kaggle/input/titanicdataset-traincsv/train.csv\n","/kaggle/input/supermarket-sales/supermarket_sales - Sheet1.csv\n","/kaggle/input/crypto/Data.csv\n","/kaggle/input/population-dataset-of-top-500-indian-cities/cities_r2.csv\n","/kaggle/input/ipldata/matches.csv\n","/kaggle/input/ipldata/deliveries.csv\n","/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","id":"2fe37740","metadata":{"papermill":{"duration":0.009232,"end_time":"2024-09-30T05:34:57.633266","exception":false,"start_time":"2024-09-30T05:34:57.624034","status":"completed"},"tags":[]},"source":["> # **Learning from**\n","\n","# 1.**IPL Dataset Practise Am** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/nowke9/ipldata\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/ipl-dataset-practise-am\n","\n","# 2.**Feature-Engineering IPL 2008 to 2020 _Am** notebook\n","* `Dataset :`  https://www.kaggle.com/datasets/patrickb1912/ipl-complete-dataset-20082020\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/feature-engineering-ipl-2008-to-2020-am\n","\n","# 3. **Visualization IPL 2008 to 2020_Am** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/patrickb1912/ipl-complete-dataset-20082020\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/visualization-ipl-2008-to-2020-am\n","\n","* `matches['city'].replace(to_replace='Bengaluru',value='Bangalore',inplace=True)` **replace Bengaluru with Banglore**\n","* `win_per_season = matches.groupby('season').winner.value_counts()` **grouby**\n","* `win_per_season_df=pd.DataFrame(columns=['season','team','no_of_time_won'])`\n","\n","     year=2008\n","     for items in win_per_season.items():   \n","       if(items[0][0]==year):     \n","            print(items)\n","            temp=pd.DataFrame({'season':[items[0][0]],'team':[items[0][1]],'no_of_time_won':[items[1]]})\n","            win_per_season_df=win_per_season_df.append(temp,ignore_index=True)           \n","            year=year+1\n","* **loc vs iloc**\n","    * loc is label-based, which means that you have to specify rows and columns based on their row and column labels.\n","    * iloc is integer position-based, so you have to specify rows and columns by their integer position values (0-based integer position). \n","    * loc[row_label, column_label]\n","    * iloc[row_position, column_position]\n","    * 1.Selecting via a single value 2.Selecting via a list of values 3.Selecting a range of data via slice 4.Selecting via conditions and callable\n","    * https://towardsdatascience.com/how-to-use-loc-and-iloc-for-selecting-data-in-pandas-bd09cb4c3d79\n","* idxmax() , idxmin() , ge() --> greater than equal to"]},{"cell_type":"markdown","id":"0f73e23e","metadata":{"papermill":{"duration":0.0094,"end_time":"2024-09-30T05:34:57.652337","exception":false,"start_time":"2024-09-30T05:34:57.642937","status":"completed"},"tags":[]},"source":["> # 4.  Learning from **Estes Park Weather EDA Practices Am** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/ncsaayali/climate-dataset\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/estes-park-weather-eda-practices-am\n","* `avg_50_75_diff =climate['Average temperature (°F)'].quantile(q=0.75)-climate['Average temperature (°F)'].quantile(q=0.5)`\n","* print('avg_50_75_diff :', round(avg_50_75_diff,2)) -- count calculate quantile"]},{"cell_type":"markdown","id":"69a51f0d","metadata":{"papermill":{"duration":0.00967,"end_time":"2024-09-30T05:34:57.671998","exception":false,"start_time":"2024-09-30T05:34:57.662328","status":"completed"},"tags":[]},"source":["> # 5. Learning from **Cryptocurrencies_Am** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/sjagkoo7/crypto\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/cryptocurrencies-am\n","* A1=df.head(3).to_dict`('list')`\n","* list_1=list_1['Currency_Name'].`to_list()`\n","* `set_xlabel,set_ylabel,set_title,legend,set_xticks,set_xticklabels,set_yticks,set_yticklabels`\n","* `xlabel,ylabel,title,xticks,yticks`\n","* `get_xticklabels,get_yticklabels`\n","* `Ex : plt.xticks([0, 1], ['Male', 'Female'])`"]},{"cell_type":"markdown","id":"6cceca93","metadata":{"papermill":{"duration":0.009164,"end_time":"2024-09-30T05:34:57.690774","exception":false,"start_time":"2024-09-30T05:34:57.68161","status":"completed"},"tags":[]},"source":["> # 6. Learning from **Boston House Prediction Dataset Practise Am** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/vikrishnan/boston-house-prices\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/boston-house-prediction-dataset-practise-am\n","> **Split Dataset for Training and Testing**\n","* `from sklearn.model_selection import train_test_split`\n","* `X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=4)`\n","> **Linear Regression**\n","* `from sklearn.linear_model import LinearRegression` -- Import library for Linear Regression\n","* `lr=LinearRegression()` -- Create a Linear regressor\n","* `lr.fit(X_train,y_train)` -- Train the model using the training sets\n","* `y_pred=lr.predict(X_test)` -- Model prediction on test data\n","> **K-Nearest Neigbour(KNN) or K Neighbours Regressor**\n","* `from sklearn.neighbors  import KNeighborsRegressor`\n","* `knr=KNeighborsRegressor(n_neighbors=5)`\n","* `knr.fit(X_train,y_train)`\n","* `y_pred_test=knr.predict(X_test)`\n","> **Random Forest Regressor**\n","* `from sklearn.ensemble import RandomForestRegressor`\n","* `rfr=RandomForestRegressor(n_estimators=100,criterion=\"squared_error\",random_state=42)`\n","* `rfr.fit(X_train,y_train)`\n","* `y_pred_test=rfr.predict(X_test)`\n","> **Gradient Boosting Regressor**\n","* `from sklearn.ensemble import GradientBoostingRegressor`\n","* `gbr=GradientBoostingRegressor(n_estimators=100,learning_rate=0.1,criterion=\"friedman_mse\",random_state=42)`\n","* `gbr.fit(X_train,y_train)`\n","* `y_pred_test=gbr.predict(X_test)`\n","> **Support Vector Regressor**\n","* `from sklearn.svm import SVR`\n","* `svc_rbf=SVR(kernel=\"rbf\")`# kernel rbf\n","* `svc_rbf.fit(X_train,y_train)`\n","* `y_pred_test=svc_rbf.predict(X_test)`\n","> **Regression model Evaluation**\n","* `from sklearn import metrics`\n","* `print('R^2:',metrics.r2_score(y_test,y_pred_test))`\n","* `print('Adjusted R^2 :',1-((1-metrics.r2_score(y_test,y_pred_test))*(len(y_test)-1))/(len(y_test)-len(X_test.columns)-1))`\n","* `print('MAE:',metrics.mean_absolute_error(y_test,y_pred_test))`\n","* `print('MSE:',metrics.mean_squared_error(y_test,y_pred_test))`\n","* `print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred_test)))`"]},{"cell_type":"markdown","id":"73141d3e","metadata":{"papermill":{"duration":0.009347,"end_time":"2024-09-30T05:34:57.709631","exception":false,"start_time":"2024-09-30T05:34:57.700284","status":"completed"},"tags":[]},"source":["> # 7. Learning from **EDA of Top 500 Indian Cities Population** notebook\n","* `Dataset : ` https://www.kaggle.com/datasets/sjagkoo7/population-dataset-of-top-500-indian-cities\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/eda-of-top-500-indian-cities-population"]},{"cell_type":"markdown","id":"65cee8ba","metadata":{"papermill":{"duration":0.009266,"end_time":"2024-09-30T05:34:57.728435","exception":false,"start_time":"2024-09-30T05:34:57.719169","status":"completed"},"tags":[]},"source":["> # 8. Learning from **Data Analysis+Regression+Classification** notebook\n","* `Dataset :`  https://www.kaggle.com/datasets/benroshan/factors-affecting-campus-placement\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/data-analysis-regression-classification\n","* **Data Wrangling**    \n","* `def transform_categorical_column(data,column_name):`\n","    * `categories=data[column_name].value_counts().index.to_list()`\n","    * `map_cat={k:v  for v,k in enumerate(categories)}`\n","    * `rev_map_cat={v:k for k,v in map_cat.items()}`\n","    * `data[column_name]=data[column_name].map(map_cat)`\n","    * `return data,map_cat,rev_map_cat`\n","*  `data, map_gender, reverse_map_gender=transform_categorical_column(data, 'gender')`\n","> **Classification- Decision Tree Classification**\n","* `from sklearn.tree import DecisionTreeClassifier`\n","* `from sklearn.metrics import accuracy_score,classification_report`\n","* `dtree=DecisionTreeClassifier(criterion=\"entropy\")`\n","* `dtree.fit(X_train,y_train)`\n","* `y_pred=dtree.predict(X_test)`\n","> **Classification - Logistic Regression**\n","* `from sklearn.linear_model import LogisticRegression`\n","* `lor=LogisticRegression()`\n","* `lor.fit(X_train,y_train)`\n","* `y_pred=lor.predict(X_test)`\n","> **Classification model Evaluation**\n","* `accuracy_score(y_test,y_pred)`\n","* `print(classification_report(y_test,y_pred))`"]},{"cell_type":"markdown","id":"359c53be","metadata":{"papermill":{"duration":0.010304,"end_time":"2024-09-30T05:34:57.748519","exception":false,"start_time":"2024-09-30T05:34:57.738215","status":"completed"},"tags":[]},"source":["> # 9. Learning from **Supermarket Sales EDA** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/aungpyaeap/supermarket-sales\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/supermarket-sales-eda\n","* **Pie chart using Matplotlib**\n","* `fig=plt.figure(figsize=(10,5))`\n","* `ax=fig.add_subplot(111)`\n","* `ax.set(title='Product line')`\n","* `plt.pie(x=sales['Product line'].value_counts(),autopct='%.1f%%',labels=sales['Product line'].value_counts().index)`\n","* `plt.show()`\n","> **Datetime**\n","* `sales['Date2']=pd.to_datetime(sales['Date1'])`\n","* `sales['Day']=sales.Date2.dt.day`\n","* `sales['Month']=sales.Date2.dt.month`\n","* `sales['Year']=sales.Date2.dt.year`\n","* `sales['Weekday']=sales.Date2.dt.dayofweek`\n","* `sales['Hour']=sales.Time1.dt.hour`"]},{"cell_type":"markdown","id":"d6101e65","metadata":{"papermill":{"duration":0.00932,"end_time":"2024-09-30T05:34:57.767484","exception":false,"start_time":"2024-09-30T05:34:57.758164","status":"completed"},"tags":[]},"source":["> # 10. Learning from **heart-failure-prediction_Am** notebok\n","* `Dataset :` https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction\n","* `Notebook :`https://www.kaggle.com/code/sjagkoo7/heart-failure-prediction-am\n","* plt.subplot(x,y,z)\n","        x - row\n","        y - column\n","        z - plot number\n","        total graph will be = x * y and z will be squence number of graph\n","* `plt.tight_layout()`\n","* `subplot() vs subplots() :` https://saturncloud.io/blog/understanding-the-differences-between-subplot-and-subplots-in-matplotlib/#:~:text=While%20both%20subplot()%20and,the%20axes%20of%20the%20plot.\n","* For not Tree based Machine Learning Algorithms the best way to go will be to use One-Hot Encoding \n","* For Tree based Machine Learning Algorithms the best way to go is with Label Encoding \n","* Using Decision Tree based Algorithm does not require feature scaling, and works great also in presence of categorical columns without ONE_HOT Encoding"]},{"cell_type":"markdown","id":"119f838f","metadata":{"papermill":{"duration":0.009325,"end_time":"2024-09-30T05:34:57.786397","exception":false,"start_time":"2024-09-30T05:34:57.777072","status":"completed"},"tags":[]},"source":["> # 11. Learning from **Apps Installs EDA** notebook\n","* `Refernces`\n","* https://www.programiz.com/python-programming/datetime/strptime\n","* https://www.programiz.com/python-programming/datetime/strftime\n","* https://jovian.com/sanketchavan5595/exploratory-data-analysis-on-google-playstore-apps\n","* `Dataset :` https://www.kaggle.com/datasets/whenamancodes/play-store-apps\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/apps-installs-eda\n","> **Convert 10,00000+ object to 1000000 number**\n","* `data['Installs']=data['Installs'].apply(lambda x:x.replace(',',''))`\n","* `data['Installs']=data['Installs'].apply(lambda x:x.replace('+',''))`\n","* `data['Installs']=pd.to_numeric(data['Installs'])`"]},{"cell_type":"markdown","id":"a1450f35","metadata":{"papermill":{"duration":0.010281,"end_time":"2024-09-30T05:34:57.806322","exception":false,"start_time":"2024-09-30T05:34:57.796041","status":"completed"},"tags":[]},"source":["> # 12. Learning from **Policy Prediction Am** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/sjagkoo7/policy\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/policy-prediction-am\n","* `import ydata_profiling`\n","* `tr_data.profile_report(title='Customer Report',progress_bar=False)`"]},{"cell_type":"markdown","id":"b8fb5a92","metadata":{"papermill":{"duration":0.009277,"end_time":"2024-09-30T05:34:57.82521","exception":false,"start_time":"2024-09-30T05:34:57.815933","status":"completed"},"tags":[]},"source":["> # 13. Learning from **Decision Tree||Random Forest||SVM** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/elikplim/car-evaluation-data-set\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/decision-tree-random-forest-svm\n","* **Ordinal Encoding**\n","* Ordinal encoding is used when the categorical data has a natural order or ranking\n","* For example, if the categories are \"Low,\" \"Medium,\" and \"High,\" ordinal encoding may assign them integer values like 1, 2, and 3, respectively.\n","* `import category_encoders as ce` \n","* `encoder=ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])` -- cols -- is pre-define variable \n","* `X=encoder.fit_transform(X)`\n","> **Confusion Matrix**\n","* `from sklearn.metrics improt confusion_matrix`\n","* `cm=confusion_matrix(y_test,y_pred_test)`\n","* `print(cm)`\n","> **Feature Importance - Using Random Forest Classifier**\n","* `feature_importance=pd.Series(randomclassifier.feature_importances_,index=X_train.columns).sort_values(ascending=False)`\n","* `feature_importance`\n","> **SVM - Support Vector Machine**\n","* Applicable for both Regression & Classification\n","* Kernel types - Linear - linear , Polynomial - , RBF - Radial Basis Function (RBF) or Gaussian Kernel - poly and Sigmoid - sigmoid\n","* `from sklearn.svm import SVC`\n","* `svm_linear= SVC(kernel='linear')` -- linear kernel\n","* `svm_linear.fit(X_train,y_train)`\n","* `y_pred_test_svm=svm_linear.predict(X_test)`"]},{"cell_type":"markdown","id":"4d98ebc6","metadata":{"papermill":{"duration":0.009524,"end_time":"2024-09-30T05:34:57.844414","exception":false,"start_time":"2024-09-30T05:34:57.83489","status":"completed"},"tags":[]},"source":["> # 14. Learning from **Binary Classification of Machine Failures S3E17** notebook\n","* `Dataset:`https://www.kaggle.com/competitions/playground-series-s3e17\n","* `Notebook:`https://www.kaggle.com/code/sjagkoo7/binary-classification-of-machine-failures-s3e17\n","* **Ensemble - Randomized Trees - Random Forest Classifier**\n","* `from sklearn.ensemble import RandomForestClassifier`\n","* `dtree=RandomForestClassifier(n_estimators=100,criterion='entropy')`\n","* `dtree.fit(X_train,y_train)`\n","* `y_pred=dtree.predict(X_test)`\n","> **Label Encoding - Used with Tree Based Algorthms**\n","* Label encoding is a simpler form of encoding where each unique category is assigned a unique integer value.\n","* `from sklearn.preprocessing import LabelEncoder`\n","* `le = LabelEncoder()`\n","* `df['Type'] = le.fit_transform(df['Type'])`\n","* `df.head()`\n","> **One Hot Encoding - Used with not Tree based  Algorithms**\n","* One-hot encoding creates binary columns for each category in the original data and represents the presence of a category with a 1 and the absence with a 0.\n","* `dummy_variables=pd.get_dummies(data, columns=categorical_columns, drop_first=False)` -- categorical_columns is all categorical column name of dataset\n","* `dummy_variables.head()`\n","> **Feature Scaling**\n","* https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n","* `from sklearn.preprocessing import RobustScaler , StandardScaler , MinMaxScaler`\n","* **RobustScaler**\n","* `scaler = RobustScaler()`\n","* `df[numerical_features] = scaler.fit_transform(df[numerical_features])`  -- numerical_features : numerical columns\n","* Use RobustScaler if you want to reduce the effects of outliers\n","* RobustScaler does not scale the data into a predetermined interval like MinMaxScaler\n","* RobustScaler transforms the feature vector by subtracting the median and then dividing by the interquartile range (75% value — 25% value)\n","* **StandardScaler**\n","* `scaler = StandardScaler()`\n","* `df[numerical_features] = scaler.fit_transform(df[numerical_features])` -- numerical_features : numerical columns\n","* StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance.\n","* **MinMaxScaler**\n","* `scaler = MinMaxScaler()`\n","* `df[numerical_features] = scaler.fit_transform(df[numerical_features])` -- numerical_features : numerical columns\n","* or each value in a feature, MinMaxScaler subtracts the minimum value in the feature and then divides by the range. The range is the difference between the original maximum and original minimum.\n","* MinMaxScaler doesn’t reduce the importance of outliers.\n","* The default range for the feature returned by MinMaxScaler is 0 to 1.\n","* **tranform vs fit_transform**\n","* `fit_transform` apply on train dataset for fitting then transform\n","* `transform` on test dataset to ensure that the scaling is consistent between the training and test data \n","* `X_train_scaled = scaler.fit_transform(X_train)`\n","* `X_test_scaled = scaler.transform(X_test)`\n","> **Handle duplicates**\n","* `duplicate_rows_data = data[data.duplicated()]`\n","* `data= data.drop_duplicates()`\n"]},{"cell_type":"markdown","id":"cf9b4d04","metadata":{"papermill":{"duration":0.009385,"end_time":"2024-09-30T05:34:57.863378","exception":false,"start_time":"2024-09-30T05:34:57.853993","status":"completed"},"tags":[]},"source":["> # 15. Learning from **Multi-Label Classification Enzyme Dataset** notebook\n","* `Dataset :`https://www.kaggle.com/competitions/playground-series-s3e18\n","* `Notebook:` https://www.kaggle.com/code/sjagkoo7/multi-label-classification-enzyme-dataset\n","* Multi-CLassification  - predict output for each target seperatley by applying suitable  binary classification aglo  then merge the output in two differnt column as like target in training dataset. \n","* **Ensemble - Boosting Methods - Gradient Boosting Classifier, AdaBoost Classifier,CatBoost Classifier**\n","* Gradient Boosting Classifier - is an ensemble machine learning algorithm that creates a strong predictive model by combining the predictions of multiple weak models, typically decision trees.\n","* CatBoost Classifier - is a machine learning algorithm that uses gradient boosting on decision trees with a special emphasis on handling categorical variables. It stands for \"Category\" and \"Boosting\".\n","* CatBoost can automatically handle categorical variables and does not require extensive data preprocessing like other machine learning algorithms.\n","* AdaBoost Classifier or Adaptive Boosting - is one of the simplest boosting algorithms.AdaBoost is best used to boost the performance of decision trees on binary classification problems. \n","> **AdaBoost Classifier Algo**\n","* `from sklearn.ensemble import AdaBoostClassifier`\n","* `model_EC1=AdaBoostClassifier(n_estimators=200, learning_rate=0.1)`\n","* `model_EC1.fit(train_df_EC1_EC2,y_train_ec1)`\n","* `y_pred_ec1=model_EC1.predict(test_df_EC1_EC2)`\n"]},{"cell_type":"markdown","id":"383f09c4","metadata":{"papermill":{"duration":0.009404,"end_time":"2024-09-30T05:34:57.882438","exception":false,"start_time":"2024-09-30T05:34:57.873034","status":"completed"},"tags":[]},"source":["># 16.Learning from **Forecasting Mini-Course Sales S3E19 - Light GBM** notebook\n","* `Dataset:` https://www.kaggle.com/competitions/playground-series-s3e19\n","* `Notebook:` https://www.kaggle.com/code/sjagkoo7/forecasting-mini-course-sales-s3e19-light-gbm\n","* **time series pandas method**\n","* resample - Convenience method for frequency conversion and resampling of time series\n","* to_period - Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency \n","* to_timestamp - Cast to DatetimeIndex of timestamps, at beginning of period.\n","> **Create a FacetGrid to make a separate line plot for each product**\n","* col_wrap -- means number of columns in grid\n","* aspect -- it will seperate the graphs from overlapping\n","* `g=sns.FacetGrid(monthly_sales_country_product,col='product',col_wrap=3,height=4,aspect=1.5)`\n","* `g.map_dataframe(sns.lineplot,x='date',y='num_sold',hue='country')`\n","* `g.set_axis_labels('Date', 'Number of Products Sold')`\n","* `g.add_legend()`\n","* `g.set_axis_labels('Date', 'Number of Products Sold')`\n","* `g.fig.suptitle('Trend of Sales Over Time for Each Country and Product')`\n","* `plt.subplots_adjust(top=0.9)` -- it will adjust the graph\n","> **Ensemble - LGBMRegressor - Light gradient boosting model**\n","* one of the best model to work with time series test data\n","* `from lightgbm import LGBMRegressor`\n","* `lgbm=LGBMRegressor(n_estimators=2000,learning_rate=0.01,random_state=42)`\n","* `lgbm.fit(X_train_scaled,y_train)`\n","* `y_pred=lgbm.predict(X_test_scaled)`"]},{"cell_type":"markdown","id":"8ca2d4d7","metadata":{"papermill":{"duration":0.009323,"end_time":"2024-09-30T05:34:57.901403","exception":false,"start_time":"2024-09-30T05:34:57.89208","status":"completed"},"tags":[]},"source":["> # 17. Learning from **Time Series Crypto Currencies** notebook \n","* `Dataset :` https://www.kaggle.com/datasets/sjagkoo7/crypto-currencies-data-set\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/time-series-crypto-currencies\n","* Stationary-  https://www.analyticsvidhya.com/blog/2021/06/statistical-tests-to-check-stationarity-in-time-series-part-1/\n","* Trend-Exponential Weighted Moving Average (EWMA)\n","    * https://towardsdatascience.com/achieving-stationarity-with-time-series-data-abd59fd8d5a0\n","    * https://medium.com/codex/simple-moving-average-and-exponentially-weighted-moving-average-with-pandas-57d4a457d363\n","* Moving Average\n","    * https://towardsdatascience.com/time-series-from-scratch-moving-averages-ma-theory-and-implementation-a01b97b60a18\n","    * https://towardsdatascience.com/how-to-forecast-with-moving-average-models-6f3c9cbba60d\n","> **Stationarity** -\n","* A Stationary series is one whose statistical properties such as mean, variance, covariance, and standard deviation do not vary with time, or these stats properties are not a function of time.\n","* **Strict Stationarity** - Satisfies the mathematical definition of a stationary process. Mean, variance & covariance are not a function of time.\n","* **Seasional Stationarity** -Series exhibiting seasonality.\n","* **Trend Stationarity** - Series exhibiting trend.\n","> **Trend Removal**\n","* *Differencing*\n","* *Moving Average Smoothing - Simple Moving Average (SMA) or Weighted Moving Average (WMA)*\n","* *Exponential Weighted Moving Average (EWMA)*\n","    * `ewma = ts_log.ewm(halflife=halflife, ignore_na=False, min_periods=0, adjust=True)` -- ts_log is timeseries column i.e. like price , age , e.t.c. Keep in mind timestamp is not the timeseries data\n","* *Seasonal Decomposition* - For both trend and Seasional removal. Techniques - Seasonal Decomposition of Time Series (STL) or Seasonal and Trend decomposition using LOESS (STL)\n","    * `from  statsmodels.tsa.seasonal import seasonal_decompose`\n","    * `decomposition = seasonal_decompose(ts_log)`\n","    * `trend = decomposition.trend`\n","    * `seasonal = decomposition.seasonal`\n","    * `residual = decomposition.resid` \n","> **Forecasting**\n","    *  *Moving Average Smoothing - Simple Moving Average (SMA) or Weighted Moving Average (WMA)* - using pandas `rolling` function\n","        * `moving_average = residual.rolling(5).mean()` -- residual - time series test data , 5 is the window size in rolling function and mean() is mandate for moving avergae calculation.       \n","> **Stationarity - Test Algorithm**\n","* **ADF - Agumented Dicky - Fuller tests** \n"," * `from statsmodels.tsa.stattools import adfuller`\n"," * `dftest=adfuller(timeseries , autolag='AIC')`\n","* **Kwiatkowski–Phillips–Schmidt–Shin (KPSS) tests**\n"," * `from statsmodels.tsa.stattools import kpss`\n"," * `dftest=kpss(timeseries , regression='c',nlags='auto')`\n","> **Stationarity or Non - stationarity - P Value**\n","* Null Hypothesis(HO) - Time Series is Non-Stationarity - Series has unit root\n","* Alternate Hypothesis(HA) -  Time Series is Stationarity - Series has not unit root\n","> **Conditions to Reject Null Hypothesis(HO)**\n","- If **Test statistic < Critical Value and p-value < 0.05** – Reject Null Hypothesis(HO), i.e., time series does not have a unit root, meaning it is *stationary*. It does not have a time-dependent structure."]},{"cell_type":"markdown","id":"4429a43b","metadata":{"papermill":{"duration":0.009691,"end_time":"2024-09-30T05:34:57.920681","exception":false,"start_time":"2024-09-30T05:34:57.91099","status":"completed"},"tags":[]},"source":["> # 18. Learning from **Predict CO2 Emissions S3EP20** notebook\n","* `Dataset :` https://www.kaggle.com/competitions/playground-series-s3e20\n","* `Notebook :`  https://www.kaggle.com/code/sjagkoo7/predict-co2-emissions-s3ep20\n","\n","* Cross-Validation(CV) - https://www.analyticsvidhya.com/blog/2021/11/top-7-cross-validation-techniques-with-python-code/\n","\n","* Imputation - https://www.analyticsvidhya.com/blog/2021/06/defining-analysing-and-implementing-imputation-techniques/\n","\n","* `pd.set_option('display.max_rows', 100)` -- setting defualt max row display 100\n","\n","* `Imputation :` Imputation is a crucial technique in data cleaning and preprocessing that involves filling in missing or incomplete values in a dataset with estimated or calculated values.The goal of imputation is to enhance the quality and integrity of the data, making it more suitable for analysis and modeling.\n","\n","* `Imputation Techniques:`\n","* `for Numerical Variables / Continuous Variable:`\n","    * Mean/Median Imputation -- Health of Horses - S3 EP22 [ XGB+LGBM+CATBOOST] notebook\n","        * `for cols in cont_cols:\n","        df[cols]=df[cols].fillna(df[cols].median())` \n","    * Arbitary Value Impuation\n","    * End of tail Imputation\n","    * Mode Imputation -- Health of Horses - S3 EP22 [ XGB+LGBM+CATBOOST] notebook\n","        * `for cols in cat_cols:\n","        df[cols]=df[cols].fillna(df[cols].mode()[0])`\n","    \n","* `for Categorical Variables:`\n","    * Frequent category Imputation -- Health of Horses - S3 EP22 [ XGB+LGBM+CATBOOST] notebook\n","         * `for cols in cat_cols:\n","        df[cols]=df[cols].fillna(df[cols].mode()[0])`\n","    * Adding a \"Missing\" category\n","    \n","*  `both (Numerical + Categorical) Variables:`\n","    * Complete Case Analysis\n","    * Adding a \"Missing\" Indicator\n","    * Random Sample Imputation\n","    \n","* `Cross-validation (CV):` Cross-validation (CV) is a fundamental technique used in machine learning to assess the performance and generalization of a model. It involves partitioning the dataset into subsets for training and testing, allowing the model to be evaluated on different subsets of data.\n","\n","* `Holdout Validation (Train-Test Split):`\n","* In this technique of cross-validation, the whole dataset is randomly partitioned into a training set and validation set. Using a rule of thumb nearly 70% of the whole dataset is used as a training set and the remaining 30% is used as the validation set.\n","* `Often used when the dataset is large.Not Suitable for an imbalanced dataset.`\n","\n","* `K-Fold Cross-Validation:`\n","* In this technique of K-Fold cross-validation, the whole dataset is partitioned into K parts of equal size. Each partition is called a “Fold“.So as we have K parts we call it K-Folds. \n","* One Fold is used as a validation set and the remaining K-1 folds are used as the training set.\n","* The technique is repeated K times until each fold is used as a validation set and the remaining folds as the training set.\n","* `The final accuracy of the model is computed by taking the mean accuracy of the k-models validation data.`\n","* `Not to be used for imbalanced datasets.Not suitable for Time Series data.`\n","\n","    * `from sklearn.tree import DecisionTreeRegressor`\n","    * `from sklearn.model_selection import cross_val_score,KFold`\n","    * `dtreg=DecisionTreeRegressor()`\n","    * `kf_dtreg=KFold(n_splits=5)`\n","    * `score_dtreg=cross_val_score(dtreg,X_dtr,y_dtr,cv=kf_dtreg)`\n","    \n","* `Stratified K-Fold Cross-Validation:`\n","* Stratified K-Fold is an enhanced version of K-Fold cross-validation which is mainly used for imbalanced datasets. Just like K-fold, the whole dataset is divided into K-folds of equal size.\n","* `Not suitable for Time Series data.`\n","\n","* `Leave Pout Cross-validation:`\n","* LeavePOut cross-validation is an exhaustive cross-validation technique, in which p-samples are used as the validation set and remaining n-p samples are used as the training set.\n","* This process is repeated till the whole dataset gets divided on the validation set of p-samples and n-p training samples.\n","* `High computation time,Not Suitable for Imbalanced dataset`\n","\n","* `Leave One Out Cross-validation:`\n","* LeaveOneOut cross-validation is an exhaustive cross-validation technique in which 1 sample point is used as a validation set and the remaining n-1 samples are used as the training set. Thus the process is repeated till every sample of the dataset is used as a validation point.\n","\n","* `Time Series ( Rolling cross-validation):`\n","* Split the data into training and validation set according to time, also called as “Forward chaining” method or rolling cross-validation.\n","* Start with a small subset of data as the training set. Based on that set we predict later data points and then check the accuracy.\n","* The Predicted samples are then included as part of the next training dataset and subsequent samples are forecasted.\n","* `Not suitable for validation of other data types.`\n","\n","* `DecisionTreeRegressor :`\n","    * `from sklearn.tree import DecisionTreeRegressor`\n","    * `dtreg=DecisionTreeRegressor()`\n","    * `dtreg.fit(X_dtr,y_dtr)`\n","    * `y_dts_pred=dtreg.predict(X_dts)`\n","\n","* `Ensemble - XGBRegressor - Extreme Gradient Boosting Regressor :`\n","    * `from xgboost import XGBRegressor`\n","    * `XGBRegressor(n_estimators=100,learning_rate=0.1,random_state=42)`\n","    * `xgb.fit(X_gbtr,y_gbtr)`\n","    * `y_gbts_pred=xgb.predict(x_gbts)`\n","\n"]},{"cell_type":"markdown","id":"6ca71731","metadata":{"papermill":{"duration":0.009379,"end_time":"2024-09-30T05:34:57.939885","exception":false,"start_time":"2024-09-30T05:34:57.930506","status":"completed"},"tags":[]},"source":["> # 19. Learning from **fuel-gas-emission NOx Prediction** notebook\n","* `Dataset:` https://www.kaggle.com/datasets/sjagkoo7/fuel-gas-emission\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/fuel-gas-emission-nox-prediction\n","> **`Pipeline :`**  `sklearn.pipeline` for model training is preferred in machine learning tasks because it offers a more organized, automated, and consistent workflow. It reduces the chances of errors and improves the maintainability of your code.\n","* `from sklearn.pipeline import Pipeline`\n","* `from sklearn.preprocessing import StandardScaler`\n","* `from sklearn.ensemble import RandomForestRegressor`\n",">  `creating the pipleine for StandardScaler and Random Forest Regressor`\n","    * `pipeline=Pipeline([`\n","    * `('scaler', StandardScaler()),`  --  feature scaling\n","    * `('rfr', RandomForestRegressor())` -- random forest regressor       \n","    * `])` \n","> `accessing feature scaling using pipeline`\n","    * `X_scaled=pipeline.named_steps['scaler'].fit_transform(X_train)`  -- `named_steps` is predefined method to access internal attribute\n","    * `X_scaled=pd.DataFrame(X_scaled,columns=X_train.columns)`\n","> `model training & prediction`\n","    * ` When we call pipleine,this function automatically applies the feature scaling and model fitting.`\n","    * `pipeline.fit(X_train,y_train)`\n","    * `y_pred=pipeline.predict(X_test)`\n","> **`Hyperparameters Tuning :`** Hyperparameter tuning, also known as hyperparameter optimization, is the process of finding the best set of hyperparameters for a machine learning model to achieve optimal performance.\n","* `Parameters :` In a ML model, parameters are the variables that the model learns from the training data. For example, in a linear regression model, the coefficients are parameters. These are learned automatically during training.\n","* `Hyperparameters:` These are settings or configurations for the learning algorithm itself. They are not learned from the data, but they control the overall behavior of the model. For example, in a neural network, the learning rate is a hyperparameter.\n","* `The Hyperparameter Tuning Process:`\n","    * `Selection of Hyperparameters:` Based on prior knowledge, experience, or experimentation, a set of hyperparameters and their respective ranges are chosen.\n","    * `Search Space:` This is the range of values each hyperparameter can take. It defines the possible combinations to be explored.\n","    * `Evaluation Metric:` This is the metric used to measure the performance of the model. It could be accuracy, mean squared error, F1-score, etc., depending on the problem.\n","    * `Search Method:`\n","        * `Grid Search:` It exhaustively searches through all possible combinations in the defined search space. It's very thorough but computationally expensive.\n","        * `Random Search:` It randomly samples combinations from the search space. It's more efficient but may not find the absolute best combination.\n","        * `Bayesian Optimization, Genetic Algorithms, etc.:` These are more advanced techniques that use probabilistic models or evolutionary strategies to find good hyperparameters.\n","    * `Cross-Validation:` The model is evaluated on a validation set (or using cross-validation) for each set of hyperparameters to avoid overfitting.\n","    * `Selection of Best Hyperparameters:` The set of hyperparameters that gives the best performance on the validation set is chosen.\n","    * `Final Model:` The model is trained on the entire training set with the best hyperparameters and evaluated on the test set.\n","> **`Random Search:`**\n","    * `Hyperparameter Grid Creation Example Random Forest Regressor with pipeline:`\n","    \n","    `param_grid = {\n","        'rfr__n_estimators': list(range(200, 2001, 200)), # Number of trees\n","        'rfr__max_features': [1.0, 'sqrt'], # Number of features to consider at each split # Changed 'auto' to 1.0 as auto' being deprecated\n","        'rfr__max_depth': list(range(10, 111, 10)) + [None], # Maximum depth of the tree\n","        'rfr__min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n","        'rfr__min_samples_leaf': [1, 2, 4],  # Minimum samples required at each leaf node\n","        'rfr__bootstrap': [True, False] # Method of selecting samples for training each tree\n","    }`\n","    * `When using a pipeline, the hyperparameters of the individual estimators (like RandomForestRegressor) should be specified using the double underscore __ notation. like 'rfr__n_estimators' here rfr is the RandomForestRegressor created in pipeline.`\n","    \n","    * `Hyperparameter Grid Creation Example Random Forest Regressor without pipeline:`\n","    \n","       ` param_grid = {\n","        'n_estimators': list(range(200, 2001, 200)), # Number of trees\n","        'max_features': ['1.0', 'sqrt'], # Number of features to consider at each split # Changed 'auto' to 1.0 as auto' being \n","        'max_depth': list(range(10, 111, 10)) + [None], # Maximum depth of the tree\n","        'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n","        'min_samples_leaf': [1, 2, 4],  # Minimum samples required at each leaf node\n","        'bootstrap': [True, False] # Method of selecting samples for training each tree\n","    }`\n","    \n","  * `Create a RandomizedSearchCV model with parameters :`\n","    * `from sklearn.model_selection import RandomizedSearchCV`\n","    * with pipeline : \n","    `random_search = RandomizedSearchCV(\n","        estimator=pipeline,            -- Pipeline with StandardScaler and RandomForestRegressor\n","        param_distributions=param_grid, -- Hyperparameter grid\n","        n_iter=10,                      -- Number of random combinations to try\n","        cv=3,                           -- 3-fold cross-validation\n","        n_jobs=-1,                      -- Use all available cores\n","        random_state=42                -- Random state for reproducibility\n","    )`\n","    \n","    * without pipeline :\n","         `random_search = RandomizedSearchCV(\n","         estimator=RandomForestRegressor(), -- RandomForestRegressor\n","         param_distributions=param_grid,     -- Hyperparameter grid\n","         n_iter=10,                          -- Number of random combinations to try\n","         cv=3,                               -- 3-fold cross-validation\n","         n_jobs=-1,                          -- Use all available cores\n","         random_state=42                     -- Random state for reproducibility\n","     )`\n","     \n","* `model training & prediction for validation dataset`\n","    * `random_search.fit(X,y)`\n","     \n","* `Print the best hyperparameters`\n","    * `print(\"Best Hyperparameters:\", random_search.best_params_)\n","       hyper_y_pred_rfr=random_search.best_estimator_.predict(X_test)\n","       hyper_y_pred_rfr`"]},{"cell_type":"markdown","id":"66c26f84","metadata":{"papermill":{"duration":0.009837,"end_time":"2024-09-30T05:34:57.959368","exception":false,"start_time":"2024-09-30T05:34:57.949531","status":"completed"},"tags":[]},"source":["> # 20. Learning from **BMI - Body Mass Index Prediction** notebook\n","* `Dataset:` https://www.kaggle.com/datasets/sjagkoo7/bmi-body-mass-index\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/bmi-body-mass-index-prediction\n","> **`Grid Search:`**\n","* `Create a GridSearchCV model with parameters :`\n","    * `from sklearn.model_selection import GridSearchCV\n","       from sklearn.ensemble import RandomForestClassifier`\n","    *  `param_grid={'n_estimators':[100,200,300], # Number of trees\n","       'max_depth':[10,20,30],  # Maximum depth of the tree\n","       'min_samples_split':[2,5,10] } # Minimum samples required to split a node`\n","       \n","    * `grid_search=GridSearchCV(estimator=rfc, # RandomForestClassifier\n","       param_grid=param_grid, # Hyperparameter grid\n","       cv=3, # 3-fold cross-validation\n","       n_jobs=-1 # Use all available cores\n","       )`\n","       \n","* training the model\n","    * `grid_search.fit(X_train,y_train)`\n","* best parameters\n","    * `print(\"Best Hyperparameters:\",grid_search.best_params_)`\n","* predicting the result\n","    * `y_pred=grid_search.best_estimator_.predict(X_test)\n","       y_pred`"]},{"cell_type":"markdown","id":"adea1d3d","metadata":{"papermill":{"duration":0.009264,"end_time":"2024-09-30T05:34:57.978227","exception":false,"start_time":"2024-09-30T05:34:57.968963","status":"completed"},"tags":[]},"source":["> # 21. Learning from **Health of Horses - S3 EP22 [ XGB+LGBM+CATBOOST]** notebook\n","* `Dataset :` https://www.kaggle.com/competitions/playground-series-s3e22/data\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/predict-health-outcomes-of-horses-s3-ep22\n","* `fig,ax=plt.subplots(1,2,figsize=(12,5)) -- 1 - rows , 2 - columns`\n","    * `ax[0] means first columns -- ax[0][0]`\n","    * `ax[1] means second columns -- ax[0][1]`"]},{"cell_type":"markdown","id":"23a3669a","metadata":{"papermill":{"duration":0.009437,"end_time":"2024-09-30T05:34:57.997317","exception":false,"start_time":"2024-09-30T05:34:57.98788","status":"completed"},"tags":[]},"source":["> # 22. Learning from **House Prices - ART - RF + Gradient Boosting** notebook\n","* `Dataset :` https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/house-prices-art-rf-gradient-boosting/notebook"]},{"cell_type":"markdown","id":"088b7823","metadata":{"papermill":{"duration":0.009376,"end_time":"2024-09-30T05:34:58.016375","exception":false,"start_time":"2024-09-30T05:34:58.006999","status":"completed"},"tags":[]},"source":["> # 23. Learning from **Statistics & Probability Credit Dataset** notebook\n","* `Dataset :` https://www.kaggle.com/datasets/sjagkoo7/credit-data\n","* `Notebook :` https://www.kaggle.com/code/sjagkoo7/statistics-probability-credit-dataset\n","> `Statistical Test` :\n","    * `T-test` : Used when comparing the means of two groups (independent samples) or when comparing the mean of a single group to a  known value.\n","        * Examples:\n","            * Comparing the mean credit amounts between two gender groups (male vs. female).\n","        * `Types of t-tests` :\n","            * `One-Sample t-test` : Used to compare the mean of a sample to a known value or theoretical mean. Example: Testing if the average score of a sample is significantly different from a population mean.\n","            * `Two-Sample t-test (or Independent Samples t-test)` : Used to compare the means of two independent groups. Example: Comparing the mean scores of two groups (e.g., treatment group vs. control group).\n","            * `Code Snippet`:\n","                * from scipy.stats import ttest_ind\n","                * credit_amount_age_below_40 = data['Credit_amount'][df['Age'] <= 40]\n","                * credit_amount_age_above_40 = data['Credit_amount'][df['Age'] > 40]\n","                * t_statistic, p_value_age_groups = ttest_ind(credit_amount_age_below_40, credit_amount_age_above_40)\n","            * `Paired (or Dependent) t-test` : Used to compare the means of two related groups (paired or matched samples). Example: Comparing the before-and-after scores of the same individuals in a study.\n","    * `Annova test (Analysis of Variance)`:Used when comparing means across three or more groups.\n","        * Examples:\n","            * Comparing mean credit amounts across different age groups.\n","        * `Code Snippet`:\n","            * from scipy.stats import f_oneway\n","            * anova_results_gender = f_oneway(data['Credit_amount'][data['Gender'] == 0],\n","                                data['Credit_amount'][data['Gender'] == 1])\n","            * p_val_gender = anova_results_gender.pvalue\n","    * `Chi-Square Test` : Used for categorical variables and tests for independence between two categorical variables.\n","        * Examples:\n","            * Assessing the relationship between gender and loan approval (categorical variables).\n","    * `Accept or Reject Null Hypothesis`:\n","        * If p-value <= alpha: Reject the null hypothesis. There is enough evidence to suggest that the effect is statistically significant.\n","        * If p-value > alpha: Fail to reject the null hypothesis. There isn't enough evidence to suggest a statistically significant effect."]},{"cell_type":"markdown","id":"cbdfeb33","metadata":{"papermill":{"duration":0.009331,"end_time":"2024-09-30T05:34:58.035289","exception":false,"start_time":"2024-09-30T05:34:58.025958","status":"completed"},"tags":[]},"source":["> # 28,29. Learning from **Titanic Survival** notebook\n","* `Notebook`:\n","    * https://www.kaggle.com/code/sjagkoo7/titanic-survival-prediction-1\n","    * https://www.kaggle.com/code/sjagkoo7/titanic-survival-prediction\n","* `Dataset`:\n","    * https://www.kaggle.com/datasets/sjagkoo7/titanic-survival\n","    * https://www.kaggle.com/datasets/hesh97/titanicdataset-traincsv\n","* **`Naive Bayes`**:\n","     * `BernoulliNB` : This classifier is suitable for binary/boolean features where each feature is represented as either 0 or 1.\n","     * `CategoricalNB` : This classifier is suitable for categorical features, meaning features that can take on a finite number of discrete values.\n","     * `ComplementNB` : It is particularly suited for imbalanced datasets, where the number of samples in each class is not balanced.\n","     * `MultinomialNB` : This classifier is suitable for multinomially distributed data, often used in text classification where features represent word counts or frequencies.\n","     * `GaussianNB` : This classifier is suitable for continuous features that are assumed to follow a Gaussian (normal) distribution.\n","         * `from sklearn.naive_bayes import BernoulliNB,CategoricalNB,ComplementNB,MultinomialNB,GaussianNB`\n","         * `model = GaussianNB()`\n","         * `gaussnb_md.fit(X_train, y_train)`\n","         * `gaussnb_pred = gaussnb_md.predict(X_test)` \n","* **`K-Nearest Neigbour(KNN) or K Neighbours Classifier`**:\n","    * `from sklearn.neighbors import KNeighborsClassifier`\n","    * `knc=KNeighborsClassifier(n_neighbors=5)`\n","    * `knc.fit(X_train,y_train)`\n","    * `y_pred_test=knc.predict(X_test)`\n","* **`Support Vector Classifier`**:\n","    * `from sklearn.svm import SVC`\n","    * `svc_md = SVC(kernel='rbf',gamma='scale',random_state=32)`\n","    * `svc_md.fit(X_train, y_train)`\n","    * `svc_pred = svc_md.predict(X_test)`"]},{"cell_type":"markdown","id":"19f2d708","metadata":{"papermill":{"duration":0.00922,"end_time":"2024-09-30T05:34:58.054207","exception":false,"start_time":"2024-09-30T05:34:58.044987","status":"completed"},"tags":[]},"source":["# 42. Learning from S3E9 || Regression of Used Car Prices notebook\n","- Notebook : https://www.kaggle.com/code/sjagkoo7/s3e9-regression-of-used-car-prices\n","- Dataset : https://www.kaggle.com/code/sjagkoo7/s3e9-regression-of-used-car-prices/input"]},{"cell_type":"markdown","id":"7f3d2dcb","metadata":{"papermill":{"duration":0.009425,"end_time":"2024-09-30T05:34:58.073283","exception":false,"start_time":"2024-09-30T05:34:58.063858","status":"completed"},"tags":[]},"source":["> # Upcoming exploration ...\n","* Cluserting\n","* Recommendation System\n","* How to Use Pre-Trained Model \n","* Model Deployment"]},{"cell_type":"markdown","id":"94e6a999","metadata":{"papermill":{"duration":0.009215,"end_time":"2024-09-30T05:34:58.092131","exception":false,"start_time":"2024-09-30T05:34:58.082916","status":"completed"},"tags":[]},"source":["> # **General**\n","* 40 of the Best Beginner-Friendly Kaggle Notebooks to Learn Exploratory Data Analysis (EDA) -https://medium.com/@ebrahimhaqbhatti516/40-of-the-best-beginner-friendly-kaggle-notebooks-to-learn-exploratory-data-analysis-eda-6e45760646aa"]},{"cell_type":"markdown","id":"a6a148e7","metadata":{"papermill":{"duration":0.009352,"end_time":"2024-09-30T05:34:58.110982","exception":false,"start_time":"2024-09-30T05:34:58.10163","status":"completed"},"tags":[]},"source":["> # **Visualization Using Seabron and Matplotlib**\n","* `sns.lineplot()`\n","* `sns.countplot()`\n","* `sns.barplot()`\n","* `sns.displot()`\n","* `sns.histplot()`\n","* `sns.catplot()`\n","* `sns.heatmap()`\n","* `sns.boxplot()`\n","* `sns.violinplot()`\n","* `sns.scatterplot()`\n","* `sns.kdeplot()`\n","* `sns.pairplot()`\n","* `plt.pie()`\n","* `plt.scatter()`\n"]},{"cell_type":"markdown","id":"192f4219","metadata":{"papermill":{"duration":0.009251,"end_time":"2024-09-30T05:34:58.129776","exception":false,"start_time":"2024-09-30T05:34:58.120525","status":"completed"},"tags":[]},"source":["> # **ML Algorithm**\n","* **Regression**\n","    * Linear Regression\n","    * K-Nearest Neigbor(KNN) or K Neighbors\n","    * Tree - DecisionTreeRegressor\n","    * Ensemble - Randomized Trees - Random Forest Regressor \n","    * Ensemble - Boosting Methods - Gradient Boosting Regressor\n","    * SVM - Support Vactor Machine - SVR , Nu-SVR , LinearSVR - R - means Regression\n","    * LGBMRegressor - Light gradient boosting model , XGBRegressor - Extreme Gradient Boosting Regressor\n","* **Classification**\n","    * Logistic Regression\n","    * Tree - Decision Tree\n","    * Ensemble - Randomized Trees - Random Forest Classifier\n","    * Ensemble - Boosting Methods - AdaBoost Classifier\n","    * SVM - Support Vector Machine - Linear,Polynomial , RBF and Sigmoid - Kernel\n","    "]},{"cell_type":"markdown","id":"5afe071e","metadata":{"papermill":{"duration":0.009591,"end_time":"2024-09-30T05:34:58.148961","exception":false,"start_time":"2024-09-30T05:34:58.13937","status":"completed"},"tags":[]},"source":["> # **Steps in a Machine Learning Project**\n","* Defining the Problem\n","* Obtaining the Source Data\n","* Preparing Data for Machine Learning Algorithms\n","    > **Data Preprocessing**\n","    * Handling Null Values || Data Cleaning\n","    * Handling Categorical Variables || One Hot Encoding | Label Encoding | Ordinal Encoding | Dummy Encoding\n","    * Feature Scaling || Feature Engineering || Feature Selection\n","    * Understanding Data Through Visualization\n","* Chossing an algorithm\n","* Builiding the Model\n","* Fine-tuning the Model\n","     > **Model Evaluation**\n","* Use the best model"]},{"cell_type":"markdown","id":"5402bed1","metadata":{"papermill":{"duration":0.009393,"end_time":"2024-09-30T05:34:58.167941","exception":false,"start_time":"2024-09-30T05:34:58.158548","status":"completed"},"tags":[]},"source":["#### \n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1815,"sourceId":3139,"sourceType":"datasetVersion"},{"datasetId":2298,"sourceId":3884,"sourceType":"datasetVersion"},{"datasetId":11657,"sourceId":16098,"sourceType":"datasetVersion"},{"datasetId":29843,"sourceId":429151,"sourceType":"datasetVersion"},{"datasetId":205965,"sourceId":451952,"sourceType":"datasetVersion"},{"datasetId":596958,"sourceId":1073629,"sourceType":"datasetVersion"},{"datasetId":1582403,"sourceId":2603715,"sourceType":"datasetVersion"},{"datasetId":2085283,"sourceId":3463095,"sourceType":"datasetVersion"},{"datasetId":2285024,"sourceId":3839000,"sourceType":"datasetVersion"},{"datasetId":2482841,"sourceId":4211520,"sourceType":"datasetVersion"},{"datasetId":3047861,"sourceId":5238437,"sourceType":"datasetVersion"},{"datasetId":3357852,"sourceId":5840790,"sourceType":"datasetVersion"},{"datasetId":3413693,"sourceId":5950251,"sourceType":"datasetVersion"},{"datasetId":3542653,"sourceId":6173779,"sourceType":"datasetVersion"},{"datasetId":3567037,"sourceId":6211729,"sourceType":"datasetVersion"},{"datasetId":990900,"sourceId":8637500,"sourceType":"datasetVersion"}],"dockerImageVersionId":30407,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":14.437332,"end_time":"2024-09-30T05:34:58.901252","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-30T05:34:44.46392","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}